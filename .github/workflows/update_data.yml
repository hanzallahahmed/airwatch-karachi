name: Fetch Karachi AQI Data

on:
  schedule:
    - cron: '0 6 */3 * *'  # Every 3 days at 6:00 AM UTC
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - 'fetch_data.py'
      - 'feature_engineering.py'  # Include this script too
      - '.github/workflows/update_data.yml'
      - 'requirements.txt'

jobs:
  update_data:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create directories
      run: |
        mkdir -p .cache
        mkdir -p data

    - name: Cache API requests
      uses: actions/cache@v4
      with:
        path: .cache
        key: ${{ runner.os }}-api-cache-${{ hashFiles('**/*.py') }}
        restore-keys: |
          ${{ runner.os }}-api-cache-

    - name: Cache existing data file
      uses: actions/cache@v4
      with:
        path: data/karachi_aqi_data.parquet
        key: ${{ runner.os }}-data-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-data-

    - name: Run data fetch
      run: python fetch_data.py

    - name: Check if data was updated
      id: check_changes
      run: |
        if [[ -f data/karachi_aqi_data.parquet ]]; then
          echo "data_exists=true" >> $GITHUB_OUTPUT
        else
          echo "data_exists=false" >> $GITHUB_OUTPUT
        fi

        - name: Commit and push if changed
      if: steps.check_changes.outputs.data_exists == 'true'
      env:
        GH_TOKEN: ${{ secrets.GH_TOKEN }}
      run: |
        git config --global user.email "action@github.com"
        git config --global user.name "GitHub Action"
        git remote set-url origin https://x-access-token:${GH_TOKEN}@github.com/${{ github.repository }}

        git add data/karachi_aqi_data.parquet
        git diff --staged --quiet || git commit -m "Update AQI data - $(date '+%Y-%m-%d %H:%M:%S UTC')"

        # Pull first to avoid non-fast-forward errors
        git pull --rebase origin main

        # Then push
        git push origin HEAD:main


    - name: Upload raw AQI data
      if: steps.check_changes.outputs.data_exists == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: karachi-aqi-data
        path: data/karachi_aqi_data.parquet
        retention-days: 30

    - name: Run feature engineering
      if: steps.check_changes.outputs.data_exists == 'true'
      run: python feature_engineering.py

    - name: Upload engineered dataset
      if: steps.check_changes.outputs.data_exists == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: karachi-aqi-engineered
        path: karachi_aqi_engineered_optimized.parquet
        retention-days: 30

    - name: Display workflow summary
      if: steps.check_changes.outputs.data_exists == 'true'
      run: |
        echo "## 📊 AQI Data Update Summary" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Data file updated and processed" >> $GITHUB_STEP_SUMMARY
        echo "- 📁 Raw: \`data/karachi_aqi_data.parquet\`" >> $GITHUB_STEP_SUMMARY
        echo "- 🛠️ Engineered: \`karachi_aqi_engineered_optimized.parquet\`" >> $GITHUB_STEP_SUMMARY
        echo "- 📅 Updated: $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "- 📦 Raw Size: $(ls -lh data/karachi_aqi_data.parquet | awk '{print $5}')" >> $GITHUB_STEP_SUMMARY
        echo "- 🎯 Final Rows: $(python -c "import pandas as pd; df = pd.read_parquet('karachi_aqi_engineered_optimized.parquet'); print(len(df))")" >> $GITHUB_STEP_SUMMARY
